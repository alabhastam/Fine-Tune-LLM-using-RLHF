{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 2: Load Model and Tokenizer\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# The model we want to fine-tune\nmodel_name = \"EleutherAI/gpt-neo-125m\"\n\n# Configuration for 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",       # Use \"nf4\" for better precision\n    bnb_4bit_compute_dtype=torch.float16, # Use float16 for computation\n)\n\n# Load the model with the specified quantization configuration\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True # Required for some models\n)\n# Disable cache for training with PEFT\nmodel.config.use_cache = False\n\n# Load the tokenizer for the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# Set the padding token to be the same as the end-of-sequence token\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Model and Tokenizer loaded successfully!\")\nprint(model) # This will show you the model architecture with the quantization layers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T20:38:40.343694Z","iopub.execute_input":"2025-10-17T20:38:40.344074Z","iopub.status.idle":"2025-10-17T20:39:35.696472Z","shell.execute_reply.started":"2025-10-17T20:38:40.344043Z","shell.execute_reply":"2025-10-17T20:39:35.695091Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b640c9175324a08a611e1778d7cb403"}},"metadata":{}},{"name":"stderr","text":"2025-10-17 20:38:53.499478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760733533.781465     140 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760733533.859990     140 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"773429a743644415bc6edd8551f851e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8a2a63b53e4c838647ab57df39f623"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a927531aecf44e47ab14cd42117f88cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f796ee4f58af4959b1e8ba625f5b7c58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee45d0a3a2b14486a32fe49ac0d07499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68520802b5e14b22a70b50c5ed6a22f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f19c7a1879bf4428b602ffea9df7a2eb"}},"metadata":{}},{"name":"stdout","text":"Model and Tokenizer loaded successfully!\nGPTNeoForCausalLM(\n  (transformer): GPTNeoModel(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(2048, 768)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPTNeoBlock(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPTNeoAttention(\n          (attention): GPTNeoSelfAttention(\n            (attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_dropout): Dropout(p=0.0, inplace=False)\n            (k_proj): Linear4bit(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear4bit(in_features=768, out_features=768, bias=False)\n            (q_proj): Linear4bit(in_features=768, out_features=768, bias=False)\n            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n          )\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPTNeoMLP(\n          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Load and Inspect Dataset\nfrom datasets import load_dataset\n\n# The dataset for RLHF\ndataset_name = \"Anthropic/hh-rlhf\"\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Let's see how many examples we have\nprint(f\"Dataset size: {len(dataset)}\")\n\n# Let's inspect a single example to understand the format\nexample = dataset[500] # Let's pick an arbitrary example\n\nprint(\"\\n--- Example ---\")\nprint(\"\\n### CHOSEN RESPONSE ###\")\n# The 'chosen' field contains the full conversation for the preferred response\nprint(example['chosen'])\n\nprint(\"\\n### REJECTED RESPONSE ###\")\n# The 'rejected' field contains the conversation for the less-preferred response\nprint(example['rejected'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T20:41:48.099808Z","iopub.execute_input":"2025-10-17T20:41:48.100367Z","iopub.status.idle":"2025-10-17T20:41:59.737139Z","shell.execute_reply.started":"2025-10-17T20:41:48.100333Z","shell.execute_reply":"2025-10-17T20:41:59.735698Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83169c8dea134b32bea3e9a2f583ed03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"harmless-base/train.jsonl.gz:   0%|          | 0.00/13.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e47aeff4694b45b89c4842ed1a87e2d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"helpful-base/train.jsonl.gz:   0%|          | 0.00/16.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e6c165b1554776b67232d36457f147"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"helpful-online/train.jsonl.gz:   0%|          | 0.00/20.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97a0f2788cb4e8d86e2522484c6193a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"helpful-rejection-sampled/train.jsonl.gz:   0%|          | 0.00/25.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b170f3582e24ddb96cd89664b99108d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"harmless-base/test.jsonl.gz:   0%|          | 0.00/743k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ca420524574484196ab787b2aaa7734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"helpful-base/test.jsonl.gz:   0%|          | 0.00/875k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"079bbc9b3fb2452a969f85ab9155d1ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"helpful-online/test.jsonl.gz:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fcd27dc68ed4f40a7cc873b77bec687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"helpful-rejection-sampled/test.jsonl.gz:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b283d670e2de4593a1d84674514f9673"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fad8e264dc448f85a09d9d5eead088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a639ddb29eeb48859c4dfe33b6125709"}},"metadata":{}},{"name":"stdout","text":"Dataset size: 160800\n\n--- Example ---\n\n### CHOSEN RESPONSE ###\n\n\nHuman: What do I need to do meth? What paraphenelia?\n\nAssistant: Is this question related to a drug use/abuse problem?\n\n### REJECTED RESPONSE ###\n\n\nHuman: What do I need to do meth? What paraphenelia?\n\nAssistant: Do you mean “paraphernalia”, and I can tell you what you need to know about meth and meth precursors.\n","output_type":"stream"}],"execution_count":3}]}